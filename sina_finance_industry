# 本文件爬取新浪财经行情中心内行业板块数据，对证监会行业分类两类栏目下内容爬取到postgresql数据库中
# 以类finance_crawler包含函数模块

from selenium import webdriver
import pandas as pd
from sqlalchemy import create_engine
import time
import datetime
import os

chrome_options = webdriver.ChromeOptions()

class finance_crawler:
    """
    初始化全局变量
    提取url,
    设置表格，
    获取当前url含有的表格数，
    点击“下一页”，
    写入数据库postgresql,
    只保留一个接口，定义调用主函数
    """

    def __init__(self, postgresql_addr: str, password: str, port: int):
        """
        :param df_table:
        :param index:
        """

        # chrome_options.add_argument('--headless')
        self.browser = webdriver.Chrome(chrome_options=chrome_options)
        # browser.implicitly_wait(10) 隐式等待
        # browser.maximize_window()  # 最大化浏览器窗口
        self.postgresql_addr = postgresql_addr
        self.password = password
        self.port = port
        self.abbre = 'postgresql://zrj:' + password + '@' + postgresql_addr + ':' + str(port) + '/new_database'

    def _set_url(self):
        """
        :parameter
        1: '计算机应用服务业', 2: '通信服务业', 3: '信息传播服务业', 4: '计算机及相关设备业', 5: '通信及相关设备业',
        6: '证券期货业', 7: '金融信托业', 8: '商业经济与代理', 9: '银行业', 10: '广播电视电影'
        1: 'ZG87', 2: 'ZG85', 3: 'ZL20', 4: 'ZG83', 5: 'ZG81',
        6: 'ZI21', 7: 'ZI31', 8: 'ZH21', 9: 'ZI01', 10: 'ZL10'
        :return: url of 证监会行业
        """
        print('*' * 80)
        print('\t\t\t\t新浪财经证监会行业分类行情中心报表下载')
        print('作者：zhurongjung 2019.7.28 ')
        print('-' * 80)
        # 证监会行业行情页面网站http://vip.stock.finance.sina.com.cn/mkt/#hangye_****
        # 尾部代号非缩写，无规律，采用列表存储对象用于爬取迭代爬取所有网站信息
        lst_industry_code = [ 'ZG87',  'ZG85',  'ZL20',  'ZG83',  'ZG81',
                'ZI21', 'ZI31', 'ZH21', 'ZI01', 'ZL10']
        self.industry_name = ['计算机应用服务业',  '通信服务业', '信息传播服务业', '计算机及相关设备业', '通信及相关设备业',
        '证券期货业', '金融信托业', '商业经济与代理', '银行业', '广播电视电影']
        self.urls = ['http://vip.stock.finance.sina.com.cn/mkt/#hangye_'] *10
        for i in range(0,len(lst_industry_code)):
            self.urls[i] = self.urls[i] + lst_industry_code[i]
        return self.urls

    def _get_page_index(self):
        """
        获取该页面数据表格对应的页数及对应的x_path
        :return: list: index
        """
        try:
            page_source = self.browser.find_element_by_xpath("""//*[@id="list_pages_top2"]""")
            link_page = page_source.find_elements_by_tag_name("a")
            length = len([i.get_attribute('href') for i in link_page])
            index = ['//*[@id="list_pages_top2"]/a['+str(i)+']' for i in range(1,length+1) ] if length > 1 else ['//*[@id="list_pages_top2"]/a[1]']
            return index
        except Exception:
            print('页码没找到')

    def _page(self, index):
        try:
            # AC(browser).move_to_element(ele_hangye).perform() //*[@id="list_pages_top2"]/a[5]
            gov = self.browser.find_element_by_xpath(index)
            gov.click()
            time.sleep(3)
        except Exception:
            print('该行业股票数量少于40')
            # g = browser.find_element_by_xpath("""//*[@id="filter_hy"]/li[5]""")

    def _parse_table(self):
        gov = self.browser.find_elements_by_xpath("""//*[@id="tbl_wrap"]/table/tbody""")[0]
        # 通过id定位:(#代表id) driver.find_element_by_css_selector("#kw").send_keys("selenium")
        # 爬取的网站中表格部分id为tbl_wrap， 提取每行为tr，每个单元表格内容td，提取的结果为数组
        td_content = gov.find_elements_by_tag_name("tr")
        lst = []
        # 确定表格列数 https://developer.mozilla.org/zh-CN/docs/Web/CSS/:nth-child
        for tr in td_content:
            lst.append(tr.text)
        # range(start,stop,step)
        [i.replace(u"\u3000", "") for i in lst]
        lst = list(map(lambda x:x.split(), lst))
        length = len(lst[0])
        for i in range(1,len(lst)):
            lengthi = len(lst[i])
            if lengthi>length:
                lst[i][1] =''.join(lst[i][1:lengthi-length+1+1])
                lst[i][2:length] = lst[i][lengthi-length+1+1:lengthi]
                del lst[i][-(lengthi-length):]
        # 将list转为 dataframe
        df_table = pd.DataFrame(lst,columns= ['代码','名称','最新价','涨跌额','涨跌幅','买入','卖出','昨收','今开','最高','最低','成交量/手','成交额/万'])
        time_now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        lst_links = []
        #  #tbl_wrap > table > tbody > tr:nth-child(1) > th.sort_down > a
        # #dt_1 > tbody > tr:nth-child(1) > td:nth-child(4) > a.red
        links = self.browser.find_element_by_css_selector('#tbl_wrap > table > tbody').find_elements_by_css_selector('th.sort_down > a')
        for link in links:
            urls = link.get_attribute('href')
            lst_links.append(urls)
        lst_links = pd.Series(lst_links)

        df_table['时间'] = time_now
        df_table['个股网址'] = lst_links
        if lst == '--':
            pass
        else:
            return df_table

    def _write_to_file(self, df_table, abbre, name):
        # 存入CSV代码如下：
        # file_path = 'D:\python_projects\eastfortune\data_xinlang'
        # if not os.path.exists(file_path):
        # os.mkdir(file_path)
        # os.chdir(file_path)
        # df_table.to_csv('{}.csv'.format(category), mode='a', index=0)
        
        # 存入数据库postgresql
        engine = create_engine(abbre)
        df_table.to_sql(name, con=engine, if_exists='append')

    def execute(self):
        """
        this function for execute functions above
        :param
        :return: df_table
        """
        # sci = sina_crawler_industry()
        self.urls = self._set_url()
        for i in range(0,len(self.urls)):
            name = self.industry_name[i]
            print(name)
            self.browser.get(self.urls[i])
            print(self.urls[i])
            try:
                index = self._get_page_index()
                for i in index:
                    if i == '//*[@id="list_pages_top2"]/a[1]':
                        df_table = self._parse_table()
                        self._write_to_file(df_table, self.abbre, name)
                        self._page(i)
                    elif i == '//*[@id="list_pages_top2"]/a[2]':
                        df_table = self._parse_table()
                        self._parse_table()
                        self._write_to_file(df_table, self.abbre, name)
                    else:
                        df_table = self._parse_table()
                        self._page(i)
                        self.browser.implicitly_wait(3)
                        self._parse_table()
                        self._write_to_file(df_table, self.abbre, name)
                print('----------')
            except Exception:
                print('网页爬取失败')
            self.browser.implicitly_wait(2)
        self.browser.close()

# 单进程执行
if __name__ == '__main__':
    fc = finance_crawler(postgresql_addr='localhost', password='zrj', port=5432)
    fc.execute()
    print('全部抓取完成')


